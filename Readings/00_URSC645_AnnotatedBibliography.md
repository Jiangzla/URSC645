### Where to start?
- [Contribute to the course Google Doc](https://docs.google.com/document/d/1elFspZUIEzc66LLatNqcZBW3WM7vJ7qKHhX6Z0YcEVo/edit?usp=sharing)
- Use Google Doc to Markdown Extension to update this page

# Annotated Bibliography

## Introduction

This annotated bibliography highlights the following ideas and concepts as discussed in the literature:

1. Definitions of terms such as reproducible, replicable, workflow, science
2. Challenges associated with reproducible research
    - Social norms vs individual goals - tradeoffs
3. Computer coding, scripting, and programming concepts
4. Appropriate software for replicable research
5. Discussions of bias (what is better science?)

## The Problem

Research is a fundamental part of science that involves a significant investment of time, systematic application of methods, and careful consideration. 
"Science demands reproducibility" (Long 2009, p. 2) therefore reproducible research is a fundamental part of science. 
However, most research is not reproducible because many of the research methods are not clearly documented and tools or software are not always available.
Additionally, within the social sciences there is a general sense while research should be reproducible, the requirements are left to individual researchers.
To overcome the gap between the demand for reproducible research and the lack the skills to build reproducibility into projects, researchers need to invest time to develop data science skills. Data science skill can help build a strong workflow to support urban and regional analytic research that is systematic, generalizable, and replicable. Researchers that invest time to develop skills for reproducible research will be able to accomplish better science in less time.

## Research Question:

_How do reproducible workflow skills affect the quality and efficiency of science?_

The research question defines the sections to focus on in the literature review. Within each article review look for definitions and ways to measure the following terms:
1. reproducible
2. workflow 
3. workflow skills
4. quality of science
5. efficiency of science

## Audience

This annotated bibliography for academic researchers interested in reproducible research.

---
# Required Reading

## Munafo et al 2017
Munafò, M. R., Nosek, B. A., Bishop, D. V., Button, K. S., Chambers, C. D., du Sert, N. P., Simonsohn, U., Wagenmakers, E., Ware, J.J., & Ioannidis, J. P. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1, 0021. https://doi.org/10.1038/s41562-016-0021 

In their article, Munafò et al. (2017) provide a strong argument for why reproducible science is important. The authors describe the potential for various forms of bias to lead to self-deception. Broadly, the areas to address are with individual scientists, the availability of large datasets and potential for rapid data analysis, and systems that do not incentivize transparent research. The authors argue that currently reproducibility and transparency are not the norms in social science, but that the benefits will lead to an eventual shift in the culture.

Practices such as blinding, pre-regristration, and continuing education in modular form are provided as ways to reduce bias, improve transparency and reproducibility. There are numerous resources that could help encourage transparency and reproducibility. 

*   Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) [http://www.prisma-statement.org/](http://www.prisma-statement.org/) 
*   Transparency and Openness Promotion Guidelines (TOPS) [https://www.cos.io/initiatives/top-guidelines](https://www.cos.io/initiatives/top-guidelines)

**Definitions:**

“A hallmark of _**scientific**_ creativity is the ability to see novel and unexpected patterns in data.” (Munafò et al., 2017, p. 1)

“Open _**science**_ refers to the process of making the content and process of producing evidence and claims transparent and accessible to others.” page 5

“_**Reproducibility**_… verification of research (including, for example, sharing data and methods).” p. 5


## Freese 2007

Freese, J. (2007). Replication standards for quantitative social science: Why not sociology?. _Sociological Methods &amp; Research_, _36_(2), 153-172.

Freese defines replicate as " the simple verification or duplication of results using the same data and analytic procedures as the original researcher." (p. 154) Freese points out within Sociology there is an implicit individualistic policy that all published research should be replicable. Freese argues that the responsibility to do replicate research should be based on a social contract.

| Individual policy | Social policy |
| --- | --- |
| If asked a researcher should be able to provide the files to replicate published results | Files to replicate published results are submitted at time of publication. |
| emphasis on trust over transparency, faith in the author, reinforcement of status, ethically obscure, expires with the individual | Reduce egalitarianism. Distributes power and access. Openness. Highlights exemplary work. |

More recent examples:

_American Sociological Review_ ([https://us.sagepub.com/en-us/nam/journal/american-sociological-review#submission-guidelines](https://us.sagepub.com/en-us/nam/journal/american-sociological-review#submission-guidelines))

- ASA's stated policy on data-sharing: "Sociologists make their data available after completion of the project or its major publications, except where proprietary agreements with employers, contractors, or clients preclude such accessibility or when it is impossible to share data and protect the confidentiality of the data or the anonymity of research participants (e.g., raw field notes or detailed information from ethnographic interviews)" (ASA Code of Ethics, 1997)

_Social Psychology Quarterly_ (In Association with American Sociological Association)

([https://us.sagepub.com/en-us/nam/social-psychology-quarterly/journal201972#submission-guidelines](https://us.sagepub.com/en-us/nam/social-psychology-quarterly/journal201972#submission-guidelines))

- SPQ encourages researchers to make their data, code, and other materials available for others to review and use. We are happy to assist authors in locating a suitable depository for these materials and will be pleased to add footnotes pointing to these deposits in the published article. Because we know that not all areas of social psychology are able to provide these materials, practically or ethically, this is not a requirement and willingness to publicly release data and other materials will have no impact on the likelihood of acceptance. (Revised January 2018)

Elsevier Journals: research data sharing webpage (https://www.elsevier.com/authors/author-services/research-data)

"A number of Elsevier journals encourage authors to submit a data statement alongside their manuscript." "Several journals support co-submission of a data article alongside your manuscript. In each case, the data article is transferred to the open access journal _Data in Brief_ for publication."

Taylor &amp; Francis offers the following standardized data sharing policies across our journals ([Journal of the American Planning Association included):](https://www.tandfonline.com/toc/rjpa20/current)

([https://authorservices.taylorandfrancis.com/understanding-our-data-sharing-policies/](https://authorservices.taylorandfrancis.com/understanding-our-data-sharing-policies/))

From 2018, Taylor &amp; Francis will be introducing new policies on data sharing….

- Basic – Journal encourages authors to share and make data open where this does not violate protection of human subjects or other valid subject privacy concerns. Authors are further encouraged to cite data and provide a data availability statement.
- Share upon reasonable request – Authors agree to make their data available upon reasonable request. It's up to the author to determine whether a request is reasonable.
- Publicly available – Authors make their data freely available to the public, under a license of their choice.
- Open data – Authors must make their data freely available to the public, under a license allowing re-use by any third party for any lawful purpose. Data shall be findable and fully accessible.
- Open and fully FAIR (Findable, accessible, interoperable and re-usable) – Authors must make their data freely available to the public, under a license allowing re-use by any third party for any lawful purpose. Additionally, data shall meet with FAIR standards as established in the relevant subject area.

## Lowndes et al 2017

"Integrating open data science practices and tools into science can save time, while also improving reproducibility for our most important collaborators: our future selves." - Lowndes et al

Lowndes, Julia S. Stewart, Benjamin D. Best, Courtney Scarborough, Jamie C. Afflerbach, Melanie R. Frazier, Casey C. O'Hara, Ning Jiang and Benjamin S. Halpern (2017). Our path to better science in less time using open data science tools. _Nature Ecology &amp; Evolution_, 1, 1-7.

### Introduction

Lowndes provides a methodological account of the open data tools that they use to improve their workflow (i.e. data preparation, modeling, coding, collaboration and sharing data).

### Argument

It is often implied that good science is science that is reproducible. One of the main challenges across disciplines is making data accessible, transparent and collaborative. The author introduces various open data science tools that can make the process of reproducibility and collaboration easier. Shifting from an individual to collaborative approach to data analysis is beneficial by allowing for analysis and interpretation to become a collaborative learning process resulting in individual and collective growth and improvement.

### Audience

An experienced community of data analysts and researchers interested in collaboration and improving methodology through collaboration.

### Content

Lowndes walks us through a number of methods and tools that her and collaborative research partners use to improve their workflow and the reproducibility of their work. Table 1 provided a summary of workflow tasks, primary open data tools and how tools have advanced to make workflow more efficient. For example, every time R Markdown output files are regenerated, the R code is rerun and the associated text and figures are also regenerated to reflect any updates to the code. Another resource they used was Git 44 as a version control system tool (which tracks changes within files and allows you to revert to previous version if needed). It takes snapshots of the changes being made line-by-line and throughout time.

1. Definition not mentioned.
2. Software usually creates barriers between researchers and restrains data sharing, because of the cost of software researchers use in their research. Further, individual data organizations place challenges in reproducing research. Workflow and sharing data organization is essential to overcome replicable research challenges.
3. Free software helps establish workflow and sharing data. Establishing workflow and sharing data improve data replicability, which cannot be based on individual data organization.
4. How free software tools and platforms can help collaborative research, improving workflow, and replicability. It uses an Ocean Health Index project as an example of replicability and collaboration research.Open sources also help reproducibility.
5. Emphasis on reproducibility, collaboration, and communication

### Argument:

Free software tools and platforms, like Github, can help collaborative research, improving workflow, and replicability, because it breaks the barriers between researchers.

Gentzkow and Shapiro 2014

- Automation Rules

(A) Automate everything that can be automated.

(B) Write a single script that executes all code from beginning to end.

- Version Control Rules

(A) Store code and data under version control.

(B) Run the whole directory before checking it back in.

- Directories Rules

(A) Separate directories by function.

(B) Separate files into inputs and outputs.

(C) Make directories portable.

- Keys Rules

(A) Store cleaned data in tables with unique, non-missing keys.

(B) Keep data normalized as far into your code pipeline as you can.

- Abstraction Rules

(A) Abstract to eliminate redundancy.

(B) Abstract to improve clarity.

(C) Otherwise, don't abstract.

- Documentation Rules

(A) Don't write documentation you will not maintain.

(B) Code should be self-documenting.

- Management Rules

(A) Manage tasks with a task management system.

(B) E-mail is not a task management system.

Long 2009

## Goodman et al 2016

Goodman, S. N., Fanelli, D., &amp; Ioannidis, J. P. (2016). What does research reproducibility mean?. _Science translational medicine_, _8_(341), 341ps12-341ps12.

1. The paper introduces a new lexicon for research reproducibility. (a) Methods reproducibility, the provision of enough detail about research procedures (e.g. how many analyses were performed) and data. (b) Results reproducibility refers to obtainings the same results from a conduct of an independent study whose procedures are as close as possible. (c) Inferential reproducibility refers to drawing the same conclusions from an either independent study or a reanalysis of the original study.
2. The article points out some of its basic terms - reproducibility, replicability, reliability, robustness, and generalizability - are not standardized, leading to both conceptual and operational confusion.
3. Not mentioned
4. Not mentioned
5. Conclusion: Ill-defined reproducibility can be improved by a clear specification of reproducibility (methods, results, or inferences) and how it affects knowledge.

### Argument

The researchers need to move toward a better understanding of the relationship between reproducibility, cumulative evidence, and the truth of scientific claims.
Rendered
Annotated Bibliography [TAMU Writing Center Guidance]
Introduction
This annotated bibliography highlights the following ideas and concepts as discussed in the literature:

Definitions of terms such as reproducible, replicable, workflow, science
Challenges associated with reproducible research
Social norms vs individual goals - tradeoffs
Computer coding, scripting, and programming concepts
Appropriate software for replicable research
Discussions of bias (what is better science?)
The Problem (see Foss and Waters 2015 ch. 4 p. 36-38)
Principal Proposition [a statement that is known to be true]:

Research is a "systematic investigation or inquiry aimed at contributing to knowledge of a theory, topic, etc., by careful consideration, observation, or study of a subject." (OED Online, 2020)

"research, n.1". OED Online. December 2020. Oxford University Press. https://www-oed-com.srv-proxy2.library.tamu.edu/view/Entry/163432?rskey=sD70JD&result=1 (accessed January 29, 2021).

Research is a fundamental part of science. Research involves a significant investment of time, application of methods, and decisions.

"Science demands reproducibility" (Long 2009, p. 2)

Interacting proposition [Cast doubt on principal proposition - insert contradictory information]:

Most research is not reproducible because many of the research methods are not clearly documented, tools (software) is not always available, the contextual natural laboratory external factors not controllable…. (something like this comeback to this point)...

Individualistic nature of research - Judanne Lennox "I struggled to get these results, you need to struggle too…" - Research needs to be new and novel - so why would you want to replicate something already done...

Speculative proposition [put principal and interacting propositions side by side and name the gap of knowledge that exists]:

Researchers lack the skills for building a strong workflow to support urban and regional analytic research that is systematic, generalizable, and replicable. Researchers that invest time to develop skills for reproducible research will be able to accomplish better science in less time.

Research Question:
How do reproducible workflow skills affect the quality and efficiency of science?
Audience
This annotated bibliography for academic researchers interested in reproducible research.

Munafo et al 2017
Schulyer Carter - "Promoting transparency helps people be more open, oral history example"

"In a high-dimensional dataset, there may be hundreds or thousands of reasonable approaches to analysing the same data...If several thousand potential analytical pipelines can be applied to high-dimensional data, the generation of false-positive findings is highly likely." (demonstrates the importance and possible difficulty of being able to reproduce a finding using the exact original method) - MCS

Pre-registration may involve conflict of interest as well, especially when the researcher needs money to do their research. For example, big pharmaceutical companies pay for the clinical trials and keep the researchers from publishing non-favorable results even if pre-registration is in place.

Outcome bias is quite common. Researchers tend to rewrite their hypotheses after analysis. (Is changing measures/variables of a concept an example of outcome bias too?)

Current academic incentive system works opposite way from encouraging people to share research data.

A panelist during a disaster conference in 2018: "If you don't want to share your data, you are an evil person." However, the question is, the system is evil and we have to play that evil game.

Freese 2007
Freese, J. (2007). Replication standards for quantitative social science: Why not sociology?. Sociological Methods & Research, 36(2), 153-172.

Freese defines replicate as " the simple verification or duplication of results using the same data and analytic procedures as the original researcher." (p. 154) Freese points out within Sociology there is an implicit individualistic policy that all published research should be replicable. Freese argues that the responsibility to do replicate research should be based on a social contract.

Individual policy	Social policy
If asked a researcher should be able to provide the files to replicate published results	Files to replicate published results are submitted at time of publication.
emphasis on trust over transparency, faith in the author, reinforcement of status, ethically obscure, expires with the individual	Reduce egalitarianism. Distributes power and access. Openness. Highlights exemplary work.
More recent examples:

American Sociological Review (https://us.sagepub.com/en-us/nam/journal/american-sociological-review#submission-guidelines)

ASA's stated policy on data-sharing: "Sociologists make their data available after completion of the project or its major publications, except where proprietary agreements with employers, contractors, or clients preclude such accessibility or when it is impossible to share data and protect the confidentiality of the data or the anonymity of research participants (e.g., raw field notes or detailed information from ethnographic interviews)" (ASA Code of Ethics, 1997)
Social Psychology Quarterly (In Association with American Sociological Association)

(https://us.sagepub.com/en-us/nam/social-psychology-quarterly/journal201972#submission-guidelines)

SPQ encourages researchers to make their data, code, and other materials available for others to review and use. We are happy to assist authors in locating a suitable depository for these materials and will be pleased to add footnotes pointing to these deposits in the published article. Because we know that not all areas of social psychology are able to provide these materials, practically or ethically, this is not a requirement and willingness to publicly release data and other materials will have no impact on the likelihood of acceptance. (Revised January 2018)
Elsevier Journals: research data sharing webpage (https://www.elsevier.com/authors/author-services/research-data)

"A number of Elsevier journals encourage authors to submit a data statement alongside their manuscript." "Several journals support co-submission of a data article alongside your manuscript. In each case, the data article is transferred to the open access journal Data in Brief for publication."

Taylor & Francis offers the following standardized data sharing policies across our journals (Journal of the American Planning Association included):

(https://authorservices.taylorandfrancis.com/understanding-our-data-sharing-policies/)

From 2018, Taylor & Francis will be introducing new policies on data sharing….

Basic – Journal encourages authors to share and make data open where this does not violate protection of human subjects or other valid subject privacy concerns. Authors are further encouraged to cite data and provide a data availability statement.
Share upon reasonable request – Authors agree to make their data available upon reasonable request. It's up to the author to determine whether a request is reasonable.
Publicly available – Authors make their data freely available to the public, under a license of their choice.
Open data – Authors must make their data freely available to the public, under a license allowing re-use by any third party for any lawful purpose. Data shall be findable and fully accessible.
Open and fully FAIR (Findable, accessible, interoperable and re-usable) – Authors must make their data freely available to the public, under a license allowing re-use by any third party for any lawful purpose. Additionally, data shall meet with FAIR standards as established in the relevant subject area.
Lowndes et al 2017
"Integrating open data science practices and tools into science can save time, while also improving reproducibility for our most important collaborators: our future selves." - Lowndes et al

Lowndes, Julia S. Stewart, Benjamin D. Best, Courtney Scarborough, Jamie C. Afflerbach, Melanie R. Frazier, Casey C. O'Hara, Ning Jiang and Benjamin S. Halpern (2017). Our path to better science in less time using open data science tools. Nature Ecology & Evolution, 1, 1-7.

Introduction
Lowndes provides a methodological account of the open data tools that they use to improve their workflow (i.e. data preparation, modeling, coding, collaboration and sharing data).

Argument
It is often implied that good science is science that is reproducible. One of the main challenges across disciplines is making data accessible, transparent and collaborative. The author introduces various open data science tools that can make the process of reproducibility and collaboration easier. Shifting from an individual to collaborative approach to data analysis is beneficial by allowing for analysis and interpretation to become a collaborative learning process resulting in individual and collective growth and improvement.

Audience
An experienced community of data analysts and researchers interested in collaboration and improving methodology through collaboration.

Content
Lowndes walks us through a number of methods and tools that her and collaborative research partners use to improve their workflow and the reproducibility of their work. Table 1 provided a summary of workflow tasks, primary open data tools and how tools have advanced to make workflow more efficient. For example, every time R Markdown output files are regenerated, the R code is rerun and the associated text and figures are also regenerated to reflect any updates to the code. Another resource they used was Git 44 as a version control system tool (which tracks changes within files and allows you to revert to previous version if needed). It takes snapshots of the changes being made line-by-line and throughout time.

Definition not mentioned.
Software usually creates barriers between researchers and restrains data sharing, because of the cost of software researchers use in their research. Further, individual data organizations place challenges in reproducing research. Workflow and sharing data organization is essential to overcome replicable research challenges.
Free software helps establish workflow and sharing data. Establishing workflow and sharing data improve data replicability, which cannot be based on individual data organization.
How free software tools and platforms can help collaborative research, improving workflow, and replicability. It uses an Ocean Health Index project as an example of replicability and collaboration research.Open sources also help reproducibility.
Emphasis on reproducibility, collaboration, and communication
Argument:
Free software tools and platforms, like Github, can help collaborative research, improving workflow, and replicability, because it breaks the barriers between researchers.



Automation Rules
(A) Automate everything that can be automated.

(B) Write a single script that executes all code from beginning to end.

Version Control Rules
(A) Store code and data under version control.

(B) Run the whole directory before checking it back in.

Directories Rules
(A) Separate directories by function.

(B) Separate files into inputs and outputs.

(C) Make directories portable.

Keys Rules
(A) Store cleaned data in tables with unique, non-missing keys.

(B) Keep data normalized as far into your code pipeline as you can.

Abstraction Rules
(A) Abstract to eliminate redundancy.

(B) Abstract to improve clarity.

(C) Otherwise, don't abstract.

Documentation Rules
(A) Don't write documentation you will not maintain.

(B) Code should be self-documenting.

Management Rules
(A) Manage tasks with a task management system.

(B) E-mail is not a task management system.

Long 2009
Jason Campos is working on this section

Goodman et al 2016
Goodman, S. N., Fanelli, D., & Ioannidis, J. P. (2016). What does research reproducibility mean?. Science translational medicine, 8(341), 341ps12-341ps12.

The paper introduces a new lexicon for research reproducibility. (a) Methods reproducibility, the provision of enough detail about research procedures (e.g. how many analyses were performed) and data. (b) Results reproducibility refers to obtainings the same results from a conduct of an independent study whose procedures are as close as possible. (c) Inferential reproducibility refers to drawing the same conclusions from an either independent study or a reanalysis of the original study.
The article points out some of its basic terms - reproducibility, replicability, reliability, robustness, and generalizability - are not standardized, leading to both conceptual and operational confusion.
Not mentioned
Not mentioned
Conclusion: Ill-defined reproducibility can be improved by a clear specification of reproducibility (methods, results, or inferences) and how it affects knowledge.
Argument
The researchers need to move toward a better understanding of the relationship between reproducibility, cumulative evidence, and the truth of scientific claims.

Want to convert another document?

Feedback
Source
Donate
Terms
Privacy
@benbalter


## The Turing Way: A Handbook for Reproducible Data Science 

The Turing Way Community, Becky Arnold, Louise Bowler, Sarah Gibson, Patricia Herterich, Rosie Higman, … Kirstie Whitaker. (2021, Nov 10). The Turing Way: A Handbook for Reproducible Data Science (Version v1.0.1). Zenodo. http://doi.org/10.5281/zenodo.5671094 Ebook link: https://the-turing-way.netlify.app/welcome

---
# Additional Resources

## Healy 2019 Plain Text Social Science
Healy 2019 Plain Text Social Science https://kieranhealy.org/files/papers/plain-person-text.pdf

Looks like an excellent resource - need to read and include here

## ICPSR Data Prep Guide
ICPSR (v6) Guide to Social Science Data Preparation and Archiving https://www.icpsr.umich.edu/files/deposit/dataprep.pdf

Inter-university Consortium for Political and Social Research (ICPSR)

## Center for Open Science
Transparency and Openness Promotion (TOP) Guidelines retrieved from https://www.cos.io/initiatives/top-guidelines

## Netflix 2019: Explained: Coding
Netflix (Oct 24, 2019) Explained: Coding https://www.imdb.com/title/tt11167964/

Covers history, abstraction and introduces machine learning and bias

## Gandrud 2018: Journal articles are advertising not research
Gandrud, C. (2018). Reproducible research with R and RStudio. Chapman and Hall/CRC. [Ebook at TAMU Library](http://proxy.library.tamu.edu/login?url=https://ebookcentral.proquest.com/lib/tamucs/detail.action?docID=4710300)

"Slideshows, journal articles, books... are the "advertising"" (p. 3)
The research is the "full software environment, code, and data that produces the results." (Buckheit and Donoho, 1995; Donoho, 2010, 385) - In Donoho quote is attributed to Claerbout 1992

Look up:
Donoho, D. L. (2010). An invitation to reproducible computational research. Biostatistics, 11(3), 385-388.  https://doi-org.srv-proxy1.library.tamu.edu/10.1093/biostatistics/kxq028

Claerbout J,  Karrenbach M. Electronic documents give reproducible research a new meaning. In: Proceedings of the 62nd Annual International Meeting of the Society of Exploration Geophysics, 1992(pg. 601-604)

Donoho, D. (2017). 50 years of data science. [Journal of Computational and Graphical Statistics](http://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf), 26(4), 745-766.

## White, E.P., Baldridge, E., Brym, Z.T., Locey, K.J., McGlinn, D.J., & Supp, S.R. (2013). Nine simple ways to make it easier to (re)use your data. Ideas in Ecology and Evolution, 6(2), 1-10. doi:10.4033/iee.2013.6b.6.f

**Audience**: Anyone creating datasets; but specific examples are for ecology & evolutionary biology scientists

**Purpose of Article**: To provide tips, recommendations, and resources on how to make your data more shareable for others

**Software**: Python & R; some other recommendations

**Key Definitions**\*:  
Reproducible - no definition  
Replicable - no definition  
workflow - no definition  
quality of science - no definition  
efficiency of science - no definition  
\*The authors provide no key definitions of words found in the article, however, the entire article is devoted to the importance of quality dataset for the purpose of reproducing, replicating, and reusing data.

**Summary**:  
_Introduction_  
Data sharing is becoming a more widely recognized important element of the scientific process for two reasons: “allowing the replication of research results” and the “reuse of meta-analyses and projects not.. intended" by the original data collectors (pg1). However, even if data sharing occurs more, the impact of data sharing will be limited if the shared data is unusable due to structure, metadata, or licensing issues.

_Tip 1 - Share your data_  
Authors note that individuals are reluctant to share their data for a variety of concerns, such as competition over publications from the dataset, lack of giving credit where credit is due, and the idea that sharing data is challenging and time intensive. The authors counter that there are two developments that address these concerns: 1) there are limitations or embargos on data sharing to address the concerns regarding competition and 2) datasets are now citable.

The authors identify four ways sharing data is beneficial to the science community:

*   “(1) the results of existing analyses to be reproduced and improved upon (Fienberg and Martin 1985, Poisot et al. 2013)
*   (2) data to be combined in meta-analyses to reach general conclusions (Fienberg and Martin 1985)
*   (3) new approaches to be applied to the data and new questions asked using it (Fienberg and Martin 1985)
*   (4) approaches to scientific inquiry that could not be considered without broad scale data sharing (Hampton et al. 2013).” (pg 2).

They also discuss how on an individual level, sharing data is beneficial, because it can increase your citation count and allows you to reuse the data in new ways in the future.

_Tip 2 - Provide Metadata_  
The authors define metadata as “information about the data, including how it was collected, what the units of measurement are, and descriptions of how to best use the data (Michener and Jones 2012)” (pg2). Metadata is important, because it provides clarity on how useful a dataset will be for a specific project, it makes the dataset easier to use, and it makes the dataset useful for longer.  

Good meta data should include:

*   “The what, when, where, and how of data collection.
*   How to find and access the data.
*   Suggestions on the suitability of the data for answering specific questions.
*   Warnings about known problems or inconsistencies in the data, e.g., general descriptions of data limitations or a column in a table to indicate the quality of individual data points.
*   Information to check that the data are properly imported, e.g., the number of rows and columns in the dataset and the total sum of numerical columns.” (pg 2)

Metadata can come in many forms, but should be "logically organized, complete, and clear enough to enable interpretation and use of data." (pg 2). The authors recommend following the metadata standards of your field, because that will help with the reusability of your dataset. Developing good metadata starts with describing your data in the planning and collection stages.

Resources for better Metadata

*   KNB Morpho
*   USGS xtme
*   FGDC workbook

_Tip 3 - Provide an unprocessed form of data_  
It is important to provide your data in its most raw form and the clean set you used. The two datasets provide future users with more flexibility. Explain the difference between the two sets in the metadata.

_Tip 4 - Use standard data formats_  
The authors suggest using a standard format to make it easier for others to use your data. Good standards include: File type, overall data structure, and file content.

4.a - Use standard file formats  
The authors suggest using a file format that is readable by most software and when possible is non-proprietary. Additionally, it should be stored in a text file, because proprietary software like Microsoft Excel can be challenging to load in other programs. It can also become old, no longer open and therefore make your data inaccessible in the future.

Additionally, the authors suggest using tables when the data doesn't have a well defined format. Naming files should be consistent, descriptive, and no spaces.

4.b - Use standard table formats  
The authors provide three recommendations on tabular data to make it more accessible when importing on most analysis software or data management systems.

*   “Each row should represent a single observation (i.e., record) and each column should represent a single variable or type of measurement (i.e., field) (Borer et al. 2009, Strasser et al. 2011, 2012).
*   Every cell should contain only a single value (Strasser et al. 2012).
*   There should be only one column for each type of information” (pg 4)

Authors note that the most common violation of these rules is when the researcher uses a cross-tab structure. See “Recommended resources from this section” on resources to address this issue.

4.c - Use standard formats within cells  
Authors provide four recommendations:  

*   “_Be consistent_. For example, be consistent in your capitalization of words, choice of delimiters, and naming conventions for variables.
*   _Avoid special characters_. Most software for storing and analyzing data works best on plain text, and accents and other special characters can make it difficult to import your data (Borer et al. 2009, Strasser et al. 2012).
*   _Avoid using your delimiter in the data itself_ (e.g., commas in the notes filed of a comma-delimited file). This can make it difficult to import your data properly. This means that if you are using commas as the decimal separator (as is often done in continental Europe) then you should use a non- comma delimiter (e.g., a tab).
*   _When working with dates use the YYYY-MM-DD format_ (i.e., follow the ISO 8601 data standard).” (pg 4)

Recommended resources from this section:

*   For naming files recommends camel case or using underscores
*   Tools to restructure cross-tab data to avoid violating the rules in 4.b: “functions in Excel, R (e.g., melt() function in the R package reshape; Wick- ham 2007), and Python (e.g., melt() function in the Pandas Python module…” (pg 4).

_Tip 5 - Use good null valves_  
The authors discuss how working with missing or empty data valves is challenging. Therefore it is important for the original researcher to indicate this missing or empty value with a good null value. The authors recommend selecting a null value that works with as many different softwares as possible and won't cause errors in the analysis. They specifically recommend leaving the cell blank. They do note there are two challenges with using blacks, which are 1) blanks are unclear to the reader if the data is missing or simply overlooked during data entry or 2) Blanks can be confused for spaces and tabs in certain situations. But it is still the best overall.

The inserted chart includes all the authors recommendations for null values.

![](https://33333.cdn.cke-cs.com/kSW7V9NHUXugvhoQeFaf/images/b62b89fa85b9e9fe43eb1937ea6c56ae18b21ab1cec6fb98.png)

_Tip 6 - Make it easy to combine your data with other datasets_  
The authors recommend using contextual data to make combining your dataset easier with other datasets. Contextual data is typically data collected in the field, but not necessarily a variable (eg location). They recommend including this data in either the Metadata or as its own column.

The authors provide a lot of information in this section specific to the fields of ecology and biology. It might benefit the reader to check their own field regarding the inclusion and formatting of contextual data.

_Tip 7 - Perform Basic Quality Control_  
The authors note that quality control is important, because it helps decrease the chances of making a mistake in your own work and makes your dataset more usable in the future. The authors note there are both basic and advanced quality control checks that individuals can run. Advanced quality control checks involve more automation; python and R can assist. The authors provide three recommendations for basic quality control:  

*   Check to make sure there are no non-numeric values in a column that should only contain numeric values
*   Confirm that empty cells are not missing a data entry and are actually representing missing data. Indicate the missing data by using the appropriate null.
*   “Check for consistency in unit of measurement, data type (e.g., numeric, character), naming scheme (e.g., taxonomy, location), etc.” (pg 7).

Lastly, the authors recommend having someone review your data, in the same way you may ask someone to review your writing.

_Tip 8 - Use an established repository_  
“For data sharing to be effective, data should be easy to find, accessible, and stored where it will be preserved for a long time (Kowalczyk and Shankar 2011).” (pg 7). Therefore store your data in a repository.  
The authors recommend looking at where other researchers in your discipline publish their data. This will help you with identifying what repositories to use and what the standards are. The authors also suggest that you should look for repositories that have clear and easy to follow instructions and standards around citing work.  
Recommend resources for finding repositories:

*   http://databib.org
*   http://re3data.org.

_Tip 9 - Use an established and open license_  
Include a license with your data that's clear on the "... the rights and responsibilities of both the people providing the data and the people using it ..." and make the licenses as open as possible (pg7). Even minor restrictions have major consequences 2 additional resources for reading. 

Recommended resources for licenses:

*   Panton Principles
*   Creative Commons Zero (CCB)

_Conclusion_  
Data sharing can be transformative for the field of science and therefore there is growing interest in it. More funders are requesting for data to be shared. However, the shift will not be impactful, if others cannot use your data. The provided tips in this article should improve your data and make it more usable for others. The authors also emphasize that these tips are also just general better data practices. These tips can be implemented at any point in the project, but its best to start using them at the start.

## Gentzko, M., & Shapiro, J.N. (2014). Code and Data for the Social Sciences: A Practitioner’s Guide. https://web.stanford.edu/~gentzkow/research/CodeAndData.pdf

_**Chapter 1 - Introduction**_  
**Audience**: empirical social scientists

**Purpose of book**: To share "insight from experts in code & data into practical terms." The whole book focused on workflow skills.

**Key Definitions**:  
Empirical social science: "Asking good questions. Digging up novel data. Designing statistical analysis. Writing up results. many of us, most of the time, what it means is writing & debugging code…” .pg 3  
reproducible - no definition  
workflow - no definition  
quality of science - no definition  
efficiency of science - no definition

**Tip**:  
If a major firm is working on a problem, there's probably a class on it at your university. Check it out.

**Summary**:  
Authors provide an observation that many social scientists do have some programming basics, but overall they lack a formal computer science training. This lack of formal training approach that social scientists use has limitations as "projects grow bigger, the problems grew nastier, and our piecemeal efforts at improving matters... proved ever more ineffective" (pg 4). Important limitations or challenges from this approach include: replication, unexpected data changes, inefficiency in the researcher code, or clarity.

_**Chapter 2 - Automation**_  
**Software**: Predominantly Stata

**Key Definitions**:  
Reproducible - no definition  
workflow - no definition  
quality of science - no definition  
efficiency of science - no definition

**Rules**:

1.  Automate everything that can be automated
2.  Write a single script that executes all code from beginning to end (authors provide Stata code recommendations for this on page 8 & 9.)

**Summary**:  
The Interactive mode of research - save as you go without thinking long term - is bad for 2 reasons:  
Replicability: With the interactive mode there is "no record of the precise steps taken" or "definition of what anything means”.  
Efficiency: In the interactive mode there are no “scripting key steps” nor a project directory therefore future analyses changes are more challenging to make.

The authors recommend automating as much as possible.

Introduces the reader to the potato chip and TV method that runs the length of the book.

_**Chapter 3 - Version Control**_  
**Recommended Resources**:

*   Version Control Resources
*   Sub Version
*   Tortviso SNV for Windows
*   Git or Bit Buckets

**Key Definitions**:  
Reproducible - no definition  
workflow - no definition  
quality of science - no definition  
efficiency of science - no definition

**Rules**:

1.  Store code and data under version control
2.  Run the whole directory before checking it back in

**Summary**:  
Using dates to indicate a file version - while a good attempt - is wrong, because 1) it is hard to know "when to ‘spawn’ a new version and when to edit the old one." (12 pg) and 2) it generates  
confusion because the naming of the file is also not clear.

The authors recommend using a version control method. On your PC, you create a repository or remote server. When you want to modify a directory, you check it out, make your edits & changes. Then you run the entire directory to make sure it works. Make corrections if needed. Once it's complete, you check the directory back in.

Version control also comes with an undo function, so it is easy to access older versions.

A common version control users are familiar with is Google Docs.
