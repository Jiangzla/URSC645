### Where to start?
- [Contribute to the course Google Doc](https://docs.google.com/document/d/1elFspZUIEzc66LLatNqcZBW3WM7vJ7qKHhX6Z0YcEVo/edit?usp=sharing)
- Use Google Doc to Markdown Extension to update this page

# Annotated Bibliography

## Introduction

This annotated bibliography highlights the following ideas and concepts as discussed in the literature:

1. Definitions of terms such as reproducible, replicable, workflow, science
2. Challenges associated with reproducible research
    - Social norms vs individual goals - tradeoffs
3. Computer coding, scripting, and programming concepts
4. Appropriate software for replicable research
5. Discussions of bias (what is better science?)

## The Problem

Research is a fundamental part of science that involves a significant investment of time, systematic application of methods, and careful consideration. 
"Science demands reproducibility" (Long 2009, p. 2) therefore reproducible research is a fundamental part of science. 
However, most research is not reproducible because many of the research methods are not clearly documented and tools or software are not always available.
Additionally, within the social sciences there is a general sense while research should be reproducible, the requirements are left to individual researchers.
To overcome the gap between the demand for reproducible research and the lack the skills to build reproducibility into projects, researchers need to invest time to develop data science skills. Data science skill can help build a strong workflow to support urban and regional analytic research that is systematic, generalizable, and replicable. Researchers that invest time to develop skills for reproducible research will be able to accomplish better science in less time.

## Research Question:

_How do reproducible workflow skills affect the quality and efficiency of science?_

The research question defines the sections to focus on in the literature review. Within each article review look for definitions and ways to measure the following terms:
1. reproducible
2. workflow 
3. workflow skills
4. quality of science
5. efficiency of science

## Audience

This annotated bibliography for academic researchers interested in reproducible research.

---
# Required Reading

## Munafo et al 2017
Munafò, M. R., Nosek, B. A., Bishop, D. V., Button, K. S., Chambers, C. D., du Sert, N. P., Simonsohn, U., Wagenmakers, E., Ware, J.J., & Ioannidis, J. P. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1, 0021. https://doi.org/10.1038/s41562-016-0021 

In their article, Munafò et al. (2017) provide a strong argument for why reproducible science is important. The authors describe the potential for various forms of bias to lead to self-deception. Broadly, the areas to address are with individual scientists, the availability of large datasets and potential for rapid data analysis, and systems that do not incentivize transparent research. The authors argue that currently reproducibility and transparency are not the norms in social science, but that the benefits will lead to an eventual shift in the culture.

Practices such as blinding, pre-regristration, and continuing education in modular form are provided as ways to reduce bias, improve transparency and reproducibility. There are numerous resources that could help encourage transparency and reproducibility. 

*   Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) [http://www.prisma-statement.org/](http://www.prisma-statement.org/) 
*   Transparency and Openness Promotion Guidelines (TOPS) [https://www.cos.io/initiatives/top-guidelines](https://www.cos.io/initiatives/top-guidelines)

**Definitions:**

“A hallmark of _**scientific**_ creativity is the ability to see novel and unexpected patterns in data.” (Munafò et al., 2017, p. 1)

“Open _**science**_ refers to the process of making the content and process of producing evidence and claims transparent and accessible to others.” page 5

“_**Reproducibility**_… verification of research (including, for example, sharing data and methods).” p. 5


## Freese 2007

Freese, J. (2007). Replication standards for quantitative social science: Why not sociology?. _Sociological Methods &amp; Research_, _36_(2), 153-172.

Freese defines replicate as " the simple verification or duplication of results using the same data and analytic procedures as the original researcher." (p. 154) Freese points out within Sociology there is an implicit individualistic policy that all published research should be replicable. Freese argues that the responsibility to do replicate research should be based on a social contract.

| Individual policy | Social policy |
| --- | --- |
| If asked a researcher should be able to provide the files to replicate published results | Files to replicate published results are submitted at time of publication. |
| emphasis on trust over transparency, faith in the author, reinforcement of status, ethically obscure, expires with the individual | Reduce egalitarianism. Distributes power and access. Openness. Highlights exemplary work. |

More recent examples:

_American Sociological Review_ ([https://us.sagepub.com/en-us/nam/journal/american-sociological-review#submission-guidelines](https://us.sagepub.com/en-us/nam/journal/american-sociological-review#submission-guidelines))

- ASA's stated policy on data-sharing: "Sociologists make their data available after completion of the project or its major publications, except where proprietary agreements with employers, contractors, or clients preclude such accessibility or when it is impossible to share data and protect the confidentiality of the data or the anonymity of research participants (e.g., raw field notes or detailed information from ethnographic interviews)" (ASA Code of Ethics, 1997)

_Social Psychology Quarterly_ (In Association with American Sociological Association)

([https://us.sagepub.com/en-us/nam/social-psychology-quarterly/journal201972#submission-guidelines](https://us.sagepub.com/en-us/nam/social-psychology-quarterly/journal201972#submission-guidelines))

- SPQ encourages researchers to make their data, code, and other materials available for others to review and use. We are happy to assist authors in locating a suitable depository for these materials and will be pleased to add footnotes pointing to these deposits in the published article. Because we know that not all areas of social psychology are able to provide these materials, practically or ethically, this is not a requirement and willingness to publicly release data and other materials will have no impact on the likelihood of acceptance. (Revised January 2018)

Elsevier Journals: research data sharing webpage (https://www.elsevier.com/authors/author-services/research-data)

"A number of Elsevier journals encourage authors to submit a data statement alongside their manuscript." "Several journals support co-submission of a data article alongside your manuscript. In each case, the data article is transferred to the open access journal _Data in Brief_ for publication."

Taylor &amp; Francis offers the following standardized data sharing policies across our journals ([Journal of the American Planning Association included):](https://www.tandfonline.com/toc/rjpa20/current)

([https://authorservices.taylorandfrancis.com/understanding-our-data-sharing-policies/](https://authorservices.taylorandfrancis.com/understanding-our-data-sharing-policies/))

From 2018, Taylor &amp; Francis will be introducing new policies on data sharing….

- Basic – Journal encourages authors to share and make data open where this does not violate protection of human subjects or other valid subject privacy concerns. Authors are further encouraged to cite data and provide a data availability statement.
- Share upon reasonable request – Authors agree to make their data available upon reasonable request. It's up to the author to determine whether a request is reasonable.
- Publicly available – Authors make their data freely available to the public, under a license of their choice.
- Open data – Authors must make their data freely available to the public, under a license allowing re-use by any third party for any lawful purpose. Data shall be findable and fully accessible.
- Open and fully FAIR (Findable, accessible, interoperable and re-usable) – Authors must make their data freely available to the public, under a license allowing re-use by any third party for any lawful purpose. Additionally, data shall meet with FAIR standards as established in the relevant subject area.

## Lowndes et al 2017

Lowndes, J. S. S., Best, B. D., Scarborough, C., Afflerbach, J. C., Frazier, M. R., O’Hara, C. C., Jiang, N., & Halpern, B. S. (2017). Our path to better science in less time using open data science tools. Nature Ecology & Evolution, 1(6), 0160. https://doi.org/10.1038/s41559-017-0160 

In Lowndes et al., 2017, the authors discuss their reasoning and process for how they reevaluated and overhauled their approach to data management and collaboration. After publishing their initial report (Ocean Health Index), when they attempted to reproduce their methods for the second report, they realized that the team varied in their approaches to data management style, and versioning, and recreating their methods easily was impossible. They elected to all use one systematic approach to data management, collectively learning the R programming language and using Git and GitHub for their file management and collaboration. They discovered that not only did the new system improve their management, it also improved their team collaboration and overall productivity because everyone had the same base knowledge about the process and language of the data management code and programs. This allowed for better vetting of the data entry and written code for errors. Over time as they incorporate more methods for improving the workflow, the overall time they must spend working on generating the annual report decreases, indicating that the methods are creating efficient work that is still accurate and does not require retooling.

"Integrating open data science practices and tools into science can save time, while also improving reproducibility for our most important collaborators: our future selves." - Lowndes et al

Lowndes, Julia S. Stewart, Benjamin D. Best, Courtney Scarborough, Jamie C. Afflerbach, Melanie R. Frazier, Casey C. O'Hara, Ning Jiang and Benjamin S. Halpern (2017). Our path to better science in less time using open data science tools. _Nature Ecology &amp; Evolution_, 1, 1-7.

### Introduction

Lowndes provides a methodological account of the open data tools that they use to improve their workflow (i.e. data preparation, modeling, coding, collaboration and sharing data).

### Argument

It is often implied that good science is science that is reproducible. One of the main challenges across disciplines is making data accessible, transparent and collaborative. The author introduces various open data science tools that can make the process of reproducibility and collaboration easier. Shifting from an individual to collaborative approach to data analysis is beneficial by allowing for analysis and interpretation to become a collaborative learning process resulting in individual and collective growth and improvement.

### Audience

An experienced community of data analysts and researchers interested in collaboration and improving methodology through collaboration.

### Content

Lowndes walks us through a number of methods and tools that her and collaborative research partners use to improve their workflow and the reproducibility of their work. Table 1 provided a summary of workflow tasks, primary open data tools and how tools have advanced to make workflow more efficient. For example, every time R Markdown output files are regenerated, the R code is rerun and the associated text and figures are also regenerated to reflect any updates to the code. Another resource they used was Git 44 as a version control system tool (which tracks changes within files and allows you to revert to previous version if needed). It takes snapshots of the changes being made line-by-line and throughout time.

1. Definition not mentioned.
2. Software usually creates barriers between researchers and restrains data sharing, because of the cost of software researchers use in their research. Further, individual data organizations place challenges in reproducing research. Workflow and sharing data organization is essential to overcome replicable research challenges.
3. Free software helps establish workflow and sharing data. Establishing workflow and sharing data improve data replicability, which cannot be based on individual data organization.
4. How free software tools and platforms can help collaborative research, improving workflow, and replicability. It uses an Ocean Health Index project as an example of replicability and collaboration research.Open sources also help reproducibility.
5. Emphasis on reproducibility, collaboration, and communication

### Argument:

Free software tools and platforms, like Github, can help collaborative research, improving workflow, and replicability, because it breaks the barriers between researchers.

Gentzkow and Shapiro 2014

- Automation Rules

(A) Automate everything that can be automated.

(B) Write a single script that executes all code from beginning to end.

- Version Control Rules

(A) Store code and data under version control.

(B) Run the whole directory before checking it back in.

- Directories Rules

(A) Separate directories by function.

(B) Separate files into inputs and outputs.

(C) Make directories portable.

- Keys Rules

(A) Store cleaned data in tables with unique, non-missing keys.

(B) Keep data normalized as far into your code pipeline as you can.

- Abstraction Rules

(A) Abstract to eliminate redundancy.

(B) Abstract to improve clarity.

(C) Otherwise, don't abstract.

- Documentation Rules

(A) Don't write documentation you will not maintain.

(B) Code should be self-documenting.

- Management Rules

(A) Manage tasks with a task management system.

(B) E-mail is not a task management system.

Long 2009

## Goodman et al 2016

Goodman, S. N., Fanelli, D., &amp; Ioannidis, J. P. (2016). What does research reproducibility mean?. _Science translational medicine_, _8_(341), 341ps12-341ps12.

1. The paper introduces a new lexicon for research reproducibility. (a) Methods reproducibility, the provision of enough detail about research procedures (e.g. how many analyses were performed) and data. (b) Results reproducibility refers to obtainings the same results from a conduct of an independent study whose procedures are as close as possible. (c) Inferential reproducibility refers to drawing the same conclusions from an either independent study or a reanalysis of the original study.
2. The article points out some of its basic terms - reproducibility, replicability, reliability, robustness, and generalizability - are not standardized, leading to both conceptual and operational confusion.
3. Not mentioned
4. Not mentioned
5. Conclusion: Ill-defined reproducibility can be improved by a clear specification of reproducibility (methods, results, or inferences) and how it affects knowledge.

### Argument

The researchers need to move toward a better understanding of the relationship between reproducibility, cumulative evidence, and the truth of scientific claims.
Rendered
Annotated Bibliography [TAMU Writing Center Guidance]
Introduction
This annotated bibliography highlights the following ideas and concepts as discussed in the literature:

Definitions of terms such as reproducible, replicable, workflow, science
Challenges associated with reproducible research
Social norms vs individual goals - tradeoffs
Computer coding, scripting, and programming concepts
Appropriate software for replicable research
Discussions of bias (what is better science?)
The Problem (see Foss and Waters 2015 ch. 4 p. 36-38)
Principal Proposition [a statement that is known to be true]:

Research is a "systematic investigation or inquiry aimed at contributing to knowledge of a theory, topic, etc., by careful consideration, observation, or study of a subject." (OED Online, 2020)

"research, n.1". OED Online. December 2020. Oxford University Press. https://www-oed-com.srv-proxy2.library.tamu.edu/view/Entry/163432?rskey=sD70JD&result=1 (accessed January 29, 2021).

Research is a fundamental part of science. Research involves a significant investment of time, application of methods, and decisions.

"Science demands reproducibility" (Long 2009, p. 2)

Interacting proposition [Cast doubt on principal proposition - insert contradictory information]:

Most research is not reproducible because many of the research methods are not clearly documented, tools (software) is not always available, the contextual natural laboratory external factors not controllable…. (something like this comeback to this point)...

Individualistic nature of research - Judanne Lennox "I struggled to get these results, you need to struggle too…" - Research needs to be new and novel - so why would you want to replicate something already done...

Speculative proposition [put principal and interacting propositions side by side and name the gap of knowledge that exists]:

Researchers lack the skills for building a strong workflow to support urban and regional analytic research that is systematic, generalizable, and replicable. Researchers that invest time to develop skills for reproducible research will be able to accomplish better science in less time.

Research Question:
How do reproducible workflow skills affect the quality and efficiency of science?
Audience
This annotated bibliography for academic researchers interested in reproducible research.

Munafo et al 2017
Schulyer Carter - "Promoting transparency helps people be more open, oral history example"

"In a high-dimensional dataset, there may be hundreds or thousands of reasonable approaches to analysing the same data...If several thousand potential analytical pipelines can be applied to high-dimensional data, the generation of false-positive findings is highly likely." (demonstrates the importance and possible difficulty of being able to reproduce a finding using the exact original method) - MCS

Pre-registration may involve conflict of interest as well, especially when the researcher needs money to do their research. For example, big pharmaceutical companies pay for the clinical trials and keep the researchers from publishing non-favorable results even if pre-registration is in place.

Outcome bias is quite common. Researchers tend to rewrite their hypotheses after analysis. (Is changing measures/variables of a concept an example of outcome bias too?)

Current academic incentive system works opposite way from encouraging people to share research data.

A panelist during a disaster conference in 2018: "If you don't want to share your data, you are an evil person." However, the question is, the system is evil and we have to play that evil game.

Freese 2007
Freese, J. (2007). Replication standards for quantitative social science: Why not sociology?. Sociological Methods & Research, 36(2), 153-172.

Freese defines replicate as " the simple verification or duplication of results using the same data and analytic procedures as the original researcher." (p. 154) Freese points out within Sociology there is an implicit individualistic policy that all published research should be replicable. Freese argues that the responsibility to do replicate research should be based on a social contract.

Individual policy	Social policy
If asked a researcher should be able to provide the files to replicate published results	Files to replicate published results are submitted at time of publication.
emphasis on trust over transparency, faith in the author, reinforcement of status, ethically obscure, expires with the individual	Reduce egalitarianism. Distributes power and access. Openness. Highlights exemplary work.
More recent examples:

American Sociological Review (https://us.sagepub.com/en-us/nam/journal/american-sociological-review#submission-guidelines)

ASA's stated policy on data-sharing: "Sociologists make their data available after completion of the project or its major publications, except where proprietary agreements with employers, contractors, or clients preclude such accessibility or when it is impossible to share data and protect the confidentiality of the data or the anonymity of research participants (e.g., raw field notes or detailed information from ethnographic interviews)" (ASA Code of Ethics, 1997)
Social Psychology Quarterly (In Association with American Sociological Association)

(https://us.sagepub.com/en-us/nam/social-psychology-quarterly/journal201972#submission-guidelines)

SPQ encourages researchers to make their data, code, and other materials available for others to review and use. We are happy to assist authors in locating a suitable depository for these materials and will be pleased to add footnotes pointing to these deposits in the published article. Because we know that not all areas of social psychology are able to provide these materials, practically or ethically, this is not a requirement and willingness to publicly release data and other materials will have no impact on the likelihood of acceptance. (Revised January 2018)
Elsevier Journals: research data sharing webpage (https://www.elsevier.com/authors/author-services/research-data)

"A number of Elsevier journals encourage authors to submit a data statement alongside their manuscript." "Several journals support co-submission of a data article alongside your manuscript. In each case, the data article is transferred to the open access journal Data in Brief for publication."

Taylor & Francis offers the following standardized data sharing policies across our journals (Journal of the American Planning Association included):

(https://authorservices.taylorandfrancis.com/understanding-our-data-sharing-policies/)

From 2018, Taylor & Francis will be introducing new policies on data sharing….

Basic – Journal encourages authors to share and make data open where this does not violate protection of human subjects or other valid subject privacy concerns. Authors are further encouraged to cite data and provide a data availability statement.
Share upon reasonable request – Authors agree to make their data available upon reasonable request. It's up to the author to determine whether a request is reasonable.
Publicly available – Authors make their data freely available to the public, under a license of their choice.
Open data – Authors must make their data freely available to the public, under a license allowing re-use by any third party for any lawful purpose. Data shall be findable and fully accessible.
Open and fully FAIR (Findable, accessible, interoperable and re-usable) – Authors must make their data freely available to the public, under a license allowing re-use by any third party for any lawful purpose. Additionally, data shall meet with FAIR standards as established in the relevant subject area.
Lowndes et al 2017
"Integrating open data science practices and tools into science can save time, while also improving reproducibility for our most important collaborators: our future selves." - Lowndes et al

Lowndes, Julia S. Stewart, Benjamin D. Best, Courtney Scarborough, Jamie C. Afflerbach, Melanie R. Frazier, Casey C. O'Hara, Ning Jiang and Benjamin S. Halpern (2017). Our path to better science in less time using open data science tools. Nature Ecology & Evolution, 1, 1-7.

Introduction
Lowndes provides a methodological account of the open data tools that they use to improve their workflow (i.e. data preparation, modeling, coding, collaboration and sharing data).

Argument
It is often implied that good science is science that is reproducible. One of the main challenges across disciplines is making data accessible, transparent and collaborative. The author introduces various open data science tools that can make the process of reproducibility and collaboration easier. Shifting from an individual to collaborative approach to data analysis is beneficial by allowing for analysis and interpretation to become a collaborative learning process resulting in individual and collective growth and improvement.

Audience
An experienced community of data analysts and researchers interested in collaboration and improving methodology through collaboration.

Content
Lowndes walks us through a number of methods and tools that her and collaborative research partners use to improve their workflow and the reproducibility of their work. Table 1 provided a summary of workflow tasks, primary open data tools and how tools have advanced to make workflow more efficient. For example, every time R Markdown output files are regenerated, the R code is rerun and the associated text and figures are also regenerated to reflect any updates to the code. Another resource they used was Git 44 as a version control system tool (which tracks changes within files and allows you to revert to previous version if needed). It takes snapshots of the changes being made line-by-line and throughout time.

Definition not mentioned.
Software usually creates barriers between researchers and restrains data sharing, because of the cost of software researchers use in their research. Further, individual data organizations place challenges in reproducing research. Workflow and sharing data organization is essential to overcome replicable research challenges.
Free software helps establish workflow and sharing data. Establishing workflow and sharing data improve data replicability, which cannot be based on individual data organization.
How free software tools and platforms can help collaborative research, improving workflow, and replicability. It uses an Ocean Health Index project as an example of replicability and collaboration research.Open sources also help reproducibility.
Emphasis on reproducibility, collaboration, and communication
Argument:
Free software tools and platforms, like Github, can help collaborative research, improving workflow, and replicability, because it breaks the barriers between researchers.

Gentzkow and Shapiro 2014

Automation Rules
(A) Automate everything that can be automated.

(B) Write a single script that executes all code from beginning to end.

Version Control Rules
(A) Store code and data under version control.

(B) Run the whole directory before checking it back in.

Directories Rules
(A) Separate directories by function.

(B) Separate files into inputs and outputs.

(C) Make directories portable.

Keys Rules
(A) Store cleaned data in tables with unique, non-missing keys.

(B) Keep data normalized as far into your code pipeline as you can.

Abstraction Rules
(A) Abstract to eliminate redundancy.

(B) Abstract to improve clarity.

(C) Otherwise, don't abstract.

Documentation Rules
(A) Don't write documentation you will not maintain.

(B) Code should be self-documenting.

Management Rules
(A) Manage tasks with a task management system.

(B) E-mail is not a task management system.

## Long (2009) - The Workflow of Data Analysis Using Stata

## Chapter 3

Problem: In this chapter, Long (2009) discusses the importance of writing and debugging do-files. Each of the tools he provides contribute to the accuracy, replicability, and efficiency of your work. While there may be a time investment in learning these tools, it will quickly be recovered as they are applied to the work being done. When conducting research, we often run into errors. We must know how to diagnose any arising issues. Long argues that the most effective way to submit commands is with do-files, which is a primary form of documentation.

The primary research question in this chapter:
- Can you (or how do you) run a do-file without running into any errors and producing the same results?

Definitions:

Do-file: Do-files are simply text files that contain your commands (Long, p. 49)

Robust: A do-file that produce exactly the same result when run at a later time or on another computer (Long, p. 51)

Legible: A do-file that is documented and formatted so that it is easy to understand what is being done (Long, p. 51)

Long suggests that the reproducibility can be measured by the following:
- Understanding how to execute commands
- Writing effective do-files
- Debugging do-files

In previous chapters, long has emphasized the importance of a good workflow. In this chapter, he states that “part of an effective workflow is taking advantage of the powerful features of your software” (Long, p. 47).

Some of the workflow skills discussed relate to writing effective do-files. More specifically, he suggest that do-files should be robust and legible. There are detailed instructions on how to make sure they fit this criteria  (Long, p. 51).

The quality of science can be assessed by ensuring the quality of do-files and knowing how to debug them in case of errors.

For the efficiency of science, he suggests having templates for do-files. “The more uniform your do-files are, the less likely you are to make errors and the easier it will be to read your output (Long, p. 63).

The Command Window: Long goes into detail about the interface of Stata and gives suggestions for how to execute commands, such as the review window, variables window, etc. (Pg. 48). One way to do that is with dialog boxes. These are drop down functions in the main Stata interface that allow you to execute commands. However, he highly suggests executing commands with a do-file. He states that 99% of the work he does in Stata uses do-files (Long, p. 49). The importance of this is that 1) it keeps a record of the commands  you ran, and 2) you can use features of your text editor, including copying, pasting, global changes, and much more.

In order to replicate and correctly interpret results, they must be robust and legible (Long, p. 51).

To make a do-file robust, they must meet the following:
- Need to be self-contained – it should use the dataset that is loaded. The do-file should not rely on something left in memory by a prior do-file or command.
- Use version control – this is done to ensure a newer version of Stata computes the commands correctly (e.g., you run a do-file that was created on an older Stata version)
- Exclude directory information – this is done so that you can run do-files on other computers without any changes since data will not always stay in the same place (you will possibly move the data over the years)

To make a do-file legible, Long suggest the following:
- Use lots of comments. For example, if you start a line with *, everything that follows is treated as a comment. You can also use // and comments as dividers.
- Use short lines. Having lines (e.g. commands) that are too long cause problems.
- Limit your abbreviations
- Have command abbreviations (see image below)
- Most importantly, be consistent! You will make fewer errors if you have a standard way to do things. This applied to the style of your do-files. 

![image](https://github.com/jason805sm/URSC645/assets/158380929/290b2ebd-7535-4433-8f85-61d43bc61053)

One of the more important suggestions Long makes it to have a template for do-files (Long, p. 63). He provides suggestions for things that should be included in your template from creating a simple do more to a more complex one (Long, p. 65).

In practice, your do-files will run into errors. It is imperative that we have solutions for how to debug them (Long, p. 68). Long provides instructions for fixing simple errors in do-files and how to fix them: it is likely that the a0 log file is open, b) log file already exists, c) incorrect variable name, d) incorrect command name, e) incorrect option, or f) missing comma before options.

He also provides steps for resolving errors (Long, p. 70). He gives general strategies that we should consider if we do not see an obvious solution for the error code encountered: Step 1 is to update Stata and user written programs (including the do-file). Step 2 is to start with a clean slate, which consists of not leaving any information in the memory, restarting Stata if necessary, rebooting your computer, etc. (Long, p. 71). Step 3 is to try other data. Step 4 is to assume everything could be wrong. Step 5 is to run the program in steps. Step 6 is to exclude parts of the do-file. Step 7 is to start over (e.g., throw out the original code, use a new file). Step 8 is to know that sometimes it is not your mistake – it could be possible that there is an error in Stata.

Long also provides instructions on debugging syntax errors (Long, p. 75, unanticipated results (Long, p. 77), and more advanced methods for debugging (Long, p. 81). If nothing else works, trace the error. His last suggestion is to ask for help so that another person try and replicate the work and find a solution (Long, p. 82).

## Hoelter, L. F., LeClere, F., Pienta, A. M., McNally, J. W., & Barlow, R. E. (2008). Using ICPSR Resources to Teach Sociology. Teaching Sociology, 36, 17-25.

Article can be found here: https://drive.google.com/file/d/1hjta3dVpWkgRUpiJp4VuQlmOh3NOdAXz/view?usp=drive_link

Problem: Over the last two decades, colleges and universities have taken on the challenge of teaching quantitative literacy (QL) to students in all majors. The social sciences are “particularly well suited to teaching students these skills” because can engage in problem solving with data
(Hoelter et al., p. 17). However, there are challenges to include quantitative literacy in undergraduate classes, such as re-working syllabi and assignments. It is also the case that students are resistant to group work, and faculty and students sometimes have difficulty separating QL from traditional mathematics as they both evoke similar stresses and anxieties. The authors suggest several ways in which the Inter-university Consortium for Political and Social Research (ICPSR) and its resources can enhance students’ quantitative literacy in coursework and across the undergraduate and graduate curricula. They also discuss several general tools available through the ICPSR website to enhance teaching and research at all levels.

The primary research question in this article:

What are the tools available to aid in the development of a curriculum that teaches undergraduate and graduate students quantitative and statistical literacy skills early in their academic careers?

Definitions:

Quantitative Literacy: The ability to understand and critically evaluate statistical results that permeate our daily lives – coupled with the ability to appreciate the contributions that statistical thinking can make public and private, professional and personal decisions (Hoelter et al., p. 18).

Information Literacy: Finding and critically evaluating information to answer questions

Statistical Literacy: Using statistics as evidence for arguments

Data Literacy: The need to understand and work with data

Workflow: Not found in this article.

Workflow skills: The authors point out several tools available for students to engage in Quantitative Literacy (QL) skills early, such as those found on the ICPSR website. Specifically, using Online Learning Center for undergraduates and Secondary Datasets for graduate students are some ways to teach workflow skills.

Quality of science: In the article, the authors state that asking students to replicate an article of interest in sociology or other fields is a valuable way to socialize students to the expectations and practices of the discipline, teaches them the norms of scientific transparency, and helps them get published early in their careers (Hoelter et al. p. 21).

Efficiency of science: Using the online tools suggested helps students to begin direct replication practices by allowing them to ask research questions that they are interested in and finding an article in a sociological journal of their interest (Hoelter et al. p. 21).

Goals, Resources, and Tools: ICPSR has prioritized making data holdings more accessible to faculty and students. When interviewing faculty in focus groups and in-depth interviews, they specified wanting the ability to 1) quickly locate relevant data that are easy to work with to demonstrate the content(s), and 2) customize any material to their own teaching approach and syllabus. This resulted in the creation of the Online Learning Center (OLC).

The OLC is a teaching/learning module that helps identify relevant, usable datasets quickly. The core of the site is made up of Data-Driven Learning Guides (DDLG’s) - it is set up like a lesson plan and makes use of “real” data from ICPSR holdings. The benefits of this are that no license is required nor do students need to learn statistical software.
 
A New Approach to Teaching Research Methods: The exposure to the scholarly process outside of some isolated journal articles is why students may have a problem coming up with good research questions that lead to the analysis of quantitative data (Hoelter et al., p. 20). Authors suggest a more useful way: “to use a model in which the students analyze not data in the traditional sense but the objects of social science discourse: the scholars, datasets, and publications that compose a network of scholarly exchange” (Hoelter et al., p. 20). A tool called Exploring Data through Research Literacy (EDRL) is available for this.

The Use of Secondary Data in Graduate Courses: Authors contend that asking students to replicate an article of interest is a valuable way to socialize students to the expectations and practices of the discipline (of sociology), and it teaches them the norms of scientific transparency and helps them get published early in their careers (Hoelter et al., p. 21). For this, there are two tools available: 1) the Publication-Related Archive, and 2) the Bibliography of Data-Related Literature.

![image](https://github.com/jason805sm/URSC645/assets/158380929/4d029dfc-ca84-4b02-8609-6f9290e991bc)

General Tools for Making Data Instruments and Analysis Easier: Another tool for students is the Survey Documentation and Analysis Interface (SDA), which was developed at UC Berkeley. There is also a variable search tool, as well as a Quick Tables to explore datasets and variables. Tutorials and teaching aids can be found throughout the ICSPR website. To foster interest in data analysis, students are awarded an Undergraduate Paper Competition for completing a research project. Those who want to learn more can apply for a summer internship at ICPSR.


## Goodman et al 2016
Goodman, S. N., Fanelli, D., & Ioannidis, J. P. (2016). What does research reproducibility mean?. Science translational medicine, 8(341), 341ps12-341ps12.

The paper introduces a new lexicon for research reproducibility. (a) Methods reproducibility, the provision of enough detail about research procedures (e.g. how many analyses were performed) and data. (b) Results reproducibility refers to obtainings the same results from a conduct of an independent study whose procedures are as close as possible. (c) Inferential reproducibility refers to drawing the same conclusions from an either independent study or a reanalysis of the original study.
The article points out some of its basic terms - reproducibility, replicability, reliability, robustness, and generalizability - are not standardized, leading to both conceptual and operational confusion.
Not mentioned
Not mentioned
Conclusion: Ill-defined reproducibility can be improved by a clear specification of reproducibility (methods, results, or inferences) and how it affects knowledge.
Argument
The researchers need to move toward a better understanding of the relationship between reproducibility, cumulative evidence, and the truth of scientific claims.

Want to convert another document?

Feedback
Source
Donate
Terms
Privacy
@benbalter


## The Turing Way: A Handbook for Reproducible Data Science 

The Turing Way Community, Becky Arnold, Louise Bowler, Sarah Gibson, Patricia Herterich, Rosie Higman, … Kirstie Whitaker. (2021, Nov 10). The Turing Way: A Handbook for Reproducible Data Science (Version v1.0.1). Zenodo. http://doi.org/10.5281/zenodo.5671094 Ebook link: https://the-turing-way.netlify.app/welcome

---
# Additional Resources

## Healy 2019 Plain Text Social Science
Healy 2019 Plain Text Social Science https://kieranhealy.org/files/papers/plain-person-text.pdf

Looks like an excellent resource - need to read and include here

## ICPSR Data Prep Guide
ICPSR (v6) Guide to Social Science Data Preparation and Archiving https://www.icpsr.umich.edu/files/deposit/dataprep.pdf

Inter-university Consortium for Political and Social Research (ICPSR)

## Center for Open Science
Transparency and Openness Promotion (TOP) Guidelines retrieved from https://www.cos.io/initiatives/top-guidelines

## Netflix 2019: Explained: Coding
Netflix (Oct 24, 2019) Explained: Coding https://www.imdb.com/title/tt11167964/

Covers history, abstraction and introduces machine learning and bias

## Gandrud 2018: Journal articles are advertising not research
Gandrud, C. (2018). Reproducible research with R and RStudio. Chapman and Hall/CRC. [Ebook at TAMU Library](http://proxy.library.tamu.edu/login?url=https://ebookcentral.proquest.com/lib/tamucs/detail.action?docID=4710300)

"Slideshows, journal articles, books... are the "advertising"" (p. 3)
The research is the "full software environment, code, and data that produces the results." (Buckheit and Donoho, 1995; Donoho, 2010, 385) - In Donoho quote is attributed to Claerbout 1992

Look up:
Donoho, D. L. (2010). An invitation to reproducible computational research. Biostatistics, 11(3), 385-388.  https://doi-org.srv-proxy1.library.tamu.edu/10.1093/biostatistics/kxq028

Claerbout J,  Karrenbach M. Electronic documents give reproducible research a new meaning. In: Proceedings of the 62nd Annual International Meeting of the Society of Exploration Geophysics, 1992(pg. 601-604)

Donoho, D. (2017). 50 years of data science. [Journal of Computational and Graphical Statistics](http://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf), 26(4), 745-766.

### **Nosek, et al., 2015**

  
Nosek, B. A., Alter, G., Banks, G. C., Borsboom, D., Bowman, S. D., Breckler, S. J., Buck, S., Chambers, C. D., Chin, G., Christensen, G., Contestabile, M., Dafoe, A., Eich, E., Freese, J., Glennerster, R., Goroff, D., Green, D. P., Hesse, B., Humphreys, M., . . . Yarkoni, T. (2015). Promoting an open research culture. Science, 348(6242), 1422-1425. https://doi.org/10.1126/science.aab2374

In Nosek et al, 2015 the authors are members of the Transparency and Openness Promotion Committee, and they advocate for the advancement of these topics in scientific research and publishing. They argue that the primary hindrance to widespread transparency among researchers is the lack of overall incentives from publishers or other bodies to make it a requirement as part of the publishing process. The members (consisting of subject matter experts, publishers, and funding bodies) created a system for citation and declaration of data availability that meets different standards for different types of research and fields, but ultimately promotes transparency. There are four levels covering multiple paper/research component topics in a transparency matrix. (Nosek et al, 2015, p. 1424) The lowest level is Zero, which is when there is little enforcement of additional transparency measures, or the current accepted level is maintained. The highest level is Three, which requires citations and public repository access of any data used, among other things. Ultimately, the system is not intended to require every scientific publication use Level Three in every category or it won’t publish, but that the matrix be used to grade the articles appropriately for each discipline to ensure the appropriate quality of transparency.

**Concepts and Definitions:**  
“**Replication** standards recognize the value of replication for independent verification of research results and identify the conditions under which replication studies will be published in the journal. To progress, science needs both innovation and self-correction; replication offers opportunities for self-correction to more efficiently identify promising research directions.” (Nosek et al, 2015, p. 1423)

“**Reproducibility** increases confidence in results and also allows scholars to learn more about what results do and do not mean. (i) Design standards increase transparency about the research process and reduce vague or incomplete reporting of the methodology. (ii) Research materials standards encourage the provision of all elements of that methodology. (iii) Data sharing standards incentivize authors to make data available in trusted repositories such as Dataverse, Dryad, the Interuniversity Consortium for Political and Social Research (ICPSR), the Open Science Framework, or the Qualitative Data Repository. (iv) Analytic methods standards do the same for the code comprising the statistical models or simulations conducted for the research.” (Nosek et al, 2015, p. 1423)
